import aiohttp
import asyncio
import csv
import re
import json
import sys
from bs4 import BeautifulSoup
from pathlib import Path

url = [
    "https://store.igefa.de/p/clean-and-clever-smartline-seifencr-me-ros-sma-91-1-sma91-1-seifencreme-12x500ml/k9UiS8fZKdKxrhxcZGuqx7",
    "https://store.igefa.de/p/kolibri-comface-mundschutz-3-lagig-typ-iir-en-14683-typ-iir-40/GyXZ2QS4JRzY9isZWkHU4V",
    "https://store.igefa.de/p/hydrovital-classic-duschgel-hydrovital-classic-duschgel-250-ml-eine-erfrischende-reinigung-fuer-jeden-tag-250-ml/Np2oJkZtF58QNBQnnLd8B5"   
    ]
CSV_OUTPUT = "scraped_data.csv"
PROGRESS_FILE = "progress.json"
SUPPLIER = "igefa Handelsgesellschaft"

# write to csv
def save_to_csv(filename, data, mode="a"):
    # Ensure the directory exists
    Path(filename).parent.mkdir(parents=True, exist_ok=True)

    file_exists = Path(filename).exists()
    with open(filename, mode, newline='', encoding="utf-8") as csvfile:
        fieldnames = data.keys()
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        if not file_exists:
            writer.writeheader()
        
        # Write the data row
        writer.writerow(data)

# Load progress
def load_progress():
    if not Path(PROGRESS_FILE).exists():
        with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
            json.dump([], f)  
        return []

    with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)

# Save progress
def save_progress(url):
    progress = load_progress()
    if url not in progress:  # Prevent duplicates
        progress.append(url)
        with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
            json.dump(progress, f)

async def fetch_html(session, url):
    async with session.get(url) as response:
        return await response.text()

# Parsing proces
async def scrape_page(session, url):
    try:
        html = await fetch_html(session, url)
        soup = BeautifulSoup(html, "html.parser")
        
        # Extract the first product element 
        product = soup.find("div", class_="LYSContainer_padding__21b81") 
        if product is None:
            print("No product found")
            return

        # 1. Product name
        name_element = product.find("h1")
        name = name_element.text.strip() if name_element else "Unknown Product Name"
        
        # 2. Breadcrumb
        breadcrumb_container = soup.find("div", class_="CategoryBreadcrumbs_sectionWrap__b5732")
        if breadcrumb_container:
            breadcrumb_links = breadcrumb_container.find_all("a")
            breadcrumb = "/".join(link.text.strip().lower().replace(" ", "-") for link in breadcrumb_links)
            breadcrumb = re.sub(r"[()]", "", breadcrumb)         
            breadcrumb = re.sub(r"-{2,}", "-", breadcrumb)  
        else:
            breadcrumb = None
            
        # 3. Execution
        execution_div = soup.find("div", class_="ProductInformation_variantInfo__5cb1d")
        execution = execution_div.find("div", class_='ant-typography ant-typography-secondary ProductCard_paragraph__03d53').text.split(": ")[1].strip() if execution_div else None

        # 4. Supplier article number
        supplier_article_number = execution_div.find("div", attrs={"data-testid": "product-information-sku"}).text.split(": ")[1].strip() if execution_div else None
        
        # 5. GTIN
        gtin_element = soup.find("div", attrs={"data-testid": "product-information-gtin"})
        EANGTIN = gtin_element.text.split(": ")[1].strip() if gtin_element else None
        
        # 6. Article number
        article_number = execution_div.text.split()[-1] if execution_div else None
        
        # 7. Product description
        description_element = product.find("div", class_="ProductDescription_description__4e5b7")
        description = description_element.text.strip() if description_element else None
        
        # 8. Supplier URL
        supplier_url = url
        
        # 9. Image URL
        image_url = (
            product.find("div", class_="image-gallery-slide")
            .find("img")['src'] if product.find("div", class_="image-gallery-slide") else None
        )
        
        # 10. Manufacturer
        manufacturer_row = product.find("tr", attrs={"data-row-key": "33"})
        manufacturer = (
            manufacturer_row.find_all("td")[1].text.strip() if manufacturer_row and len(manufacturer_row.find_all("td")) > 1 else None
        )
        
        # 11. Additional description
        benefits_div = soup.find('div', class_='ProductBenefits_productBenefits__1b77a')
        additional_description = ', '.join(li.get_text() for li in benefits_div.find_all('li')) if benefits_div else None

        # Collecting product data
        product_data = {
            "Product Name": name,
            "Original Data Column 1 (Breadcrumb)": breadcrumb,
            "Original Data Column 2 (Ausf√ºhrung)": execution,
            "Supplier Article Number": supplier_article_number,
            "EAN/GTIN": EANGTIN if EANGTIN else None,  
            "Article Number": article_number,
            "Product Description": description,
            "Supplier": SUPPLIER,
            "Supplier-URL": supplier_url,
            "Product Image URL": image_url,
            "Manufacturer": manufacturer,
            "Original Data Column 3 (Add. Description)": additional_description if additional_description else None,
        }
        
        save_progress(url)
        save_to_csv(CSV_OUTPUT, product_data)
        print(f"Successfully scraped {url}")
    except Exception as e:
        print(f"Error scraping {url}: {e}")
        save_progress(url)

# run main scrape proces
async def main():
    # Load previously scraped URLs
    scraped_urls = load_progress()
    async with aiohttp.ClientSession() as session:
        tasks = []
        for link in url:
            if link not in scraped_urls:  # Only scrape if not already done
                tasks.append(scrape_page(session, link))
            else:
                print(f"Already scraped {link}, skipping...")
        await asyncio.gather(*tasks)

# start program
if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nProgram interrupted! Exiting...")
        sys.exit(0)
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        sys.exit(1)


